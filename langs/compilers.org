

may overlap with [programming-languages], [programming-concepts], and possibly more..


isn't the compiler the language itself?

* process

systems langs (C, C++, rust, go, zig, etc.) -> target platform assembly
  - gcc, msvc, go build, etc.

high-level lang -> llvm ir -> target platform assembly
  - via llvm

high-level lang (optional: -> LLVM for optimizing) -> generated C (to link to consoles, mobile) -> target platform assembly
  - LLVM dropped C backend in v3.1  
    - this project revived it https://github.com/JuliaComputingOSS/llvm-cbe
    - however, this sounds impossible for any non-trivial program

high-level lang -> byte-code -> VM (-> machine code?)
  - TODO: how do VMs translate to machine code??
  - byte-code may be pre-compiled
  - java, c#, haxe/hashlink, elixir, etc.
  - embedded langs

high-level lang -> interpreter  
      
** TODO merge from another note

high level -> Assembly -> Machine
wages = 5; -> load wages; store 5; -> 100100 100010

compiler -> assembler

source code -> object program
linker - combines libraries with the object program

editor -> compile -> link -> load -> execute

build/make - link object code with resources to create an executable

  

  
* platform-specific

WARNING: for iOS and many consoles (except switch?), "cannot execute writeable regions", thus cannot use JIT compilation

mobile
  - iOS uses C(/objective-C)
    - yet somehow haxe's C++ could be used..??
  - Android uses Java
    - mainly for ui crap, for game dev, they just need access to the renderer..
      - not sure if java's C interop (jni) is used..

consoles:
  - all consoles are targeted via C/C++ compilers
    - as a direct path from high-level language to console hardware architecture is crazy..\


* some basics

from the end of the [[re-writing the ruby parser]] article

a *parser* is the part of the programming langauge that reads source code and converts it into a format that can be understood by the runtime. At the high level, this involves creating a tree structure that represents the flow of the program. When looking at source code, you can often see this tree structure in the indentation of code.

TODO: just finish and note that paragraph under background, i've read the rest..



* AOT vs JIT

"It is not as simple as only being about dynamic typed either. I think *for the most cases when Julia or other languages comes very close or even surpass C in performance it is because of just in time compilation*. Synthetic tests are great and teach you a lot about CS. However they usually fail to translate into the real world.

*Speed all comes down to efficient machine code. GCC or Clang are great compilers but they can only make optimization assumptions at compile time. Llvm with JIT can optimize at runtime.*

A practical but naive example, if you have a large program that can do many things but you will just call one of the program's function that perform some work 1 milion times in a for loop. Somewhat simplified Julia will only compile and optimize that specific function. In the C case GCC or Clang can't know that you will only use one of the functions at compile time and will be forced to create a much larger binary that is also optimized over the whole program and not the single function.

When you run the program Julia will incur some cost in the beginning since it is not pre-compiled but llvm will probably have done a better job at writing your function in less instructions. Those less instructions per call will sum up and you end up with a "quicker" result.

This does not make Julia or JIT superior though. The above use pattern isn't that common. *You are paying the JIT compilation time so there needs to be enough iterations to pay it off. Profile guided optimization would probably have told GCC to create just an as efficient machine code for the C case too*.

In reality if you have a use case to do some relatively small task over and over again people will usually use C/C++ anyways and write good code that ends up in efficient assembly code (just look at the output and update the C/C++ code till the assembly looks good). E.g. the Linux kernel. Otherwise GPUs and FPGAs are quickly improving now and becoming much cheaper to use to offer even more speed up for some tasks.
  - https://www.reddit.com/r/Julia/comments/77shs6/eli5_how_is_julia_so_fast/
  - basically, *a JIT can help greatly with bad code, but AOT requires that you write code well, profile/optimize yourself*

** the problem of dynamic typing

"Computers are fast when you tell them exactly what to do. With C, you tell your computer exactly "here are two integers, add them using the integer addition". Since the CPU has built-in hardware for adding integers, it knows exactly what to do, and does it very quickly.

In an interpreted language like Python, you tell the computer "here are two variables, add them". But the CPU has no concept of variables, so before the CPU can add anything, Python first has to figure out what these variables contain. If they are integers, perform integer addition, if they are floats, perform floating point addition, if they are int and float, convert the int to a float, then perform a floating point addition. Etc. Python has to do this for every single operation and every single variable. This is why Python is slower than C.

Julia is kind of in the middle between Python and C. Julia can't really know whether a variable is a float or an int either (unless you tell it explicitly), but: it cleverly plans ahead whenever you call a function. When you call a function with some arguments, all the function arguments are known. Julia then goes ahead and looks at every operation inside the function and figures out the exact CPU instructions necessary for these particular arguments! Once the exact instructions are known, they can be executed quickly. That's why a function takes longer the first time you call it: on the first call, Julia figures out all the types of all the variables and compiles it all into fast, exact CPU instructions. The second and third time, (if the argument types didn't change), it will cleverly reuse that code, and is blazing fast."
  - https://www.reddit.com/r/Julia/comments/77shs6/eli5_how_is_julia_so_fast/


* ruby

because it's possibly the most complex language (to parse)

https://ruby-compilers.com/
  - this is a great little web-site, full of gems!

** re-writing the ruby parser

TODO: link goes here..
  - dunno how to paste from system clipboard..
  - ohh right, there was a little output in the terminal about this..!!

CRuby's original parse.y was 14k loc!

as of 2023, there were 12 actively maintained parsers, 6 run-times, 6 tools (tree-sitter, etc.), creating a very fractured eco-system, as libs actually use different parsers, and as each time ruby updates, so do the parsers. At the time, only 2 non reference (CRuby) parsers supported pattern matching completely correct.

7 of the top 10 parsers use hand-written recursive descent parsers, and only 3 scripting langauges still maintain a parser generated by Bison (LR parsers), which is meant for context-free grammars. Ruby, and Python too, at times require a bit of context, context-sensitive. In order for Bison to parse Cruby, a whole bunch of context/logic/state has been pushed into the lexer. Most of the programming community has come to the same conclusion about their own parsers, and therefore have moved on to recursive descent. Also, it's possibly to reap the benefits of error-recovery.

in a small todo file by Matz on 0.95, he wrote in one line: hand-written parser(recursive descent)

re-designed and basically standardized the ruby syntax tree, for the first time. Also created serialization api so that other implementations such as JRuby and TruffleRuby can use and deserialize the syntax tree. With a standard syntax tree, then the community can start building tools that will work acorss all implementations.

error tolerance works via recovery, filling in missing keywords and nodes (parts of expressions)

the new perser ships with a language server and vs-code plugin

it has no dependencies: it is self-contained. This allows languages with good ffi/bindgen (rust/zig) to access tthe parser, and enable people to write ruby tooling in other languages.

it can parse 50k files from shopify in 5.5 seconds with 11mb max memory!

in order to support libs that use other parsers, and thus programs that use those libs, they began experimenting generating the syntax trees that other parsers create, which results in better performance
 
** rubinius

https://ruby-compilers.com/rubinius/

  - this compiler and the history of it is wild
    - seemed to start as a generate C
      - "the start of a process to translate Ruby ASTs to C code. The inline gem may have being used to experiment with generated C and supporting runtime routines."
    - then a meta-circular (use same langauge) approach: "The initial idea was to have this VM just as part of a bootstrap process, and in time it would be re-implemented in a restricted dialect of Ruby amenable to type inference and which could be statically translated to C.", similar to pypy (and rpython)
    - then had a *[[https://github.com/ruby-compiler-survey/rubinius/commit/855846c8850bc22ac1b90e1c5bfbdb5c517efda][native assembler library]]*
      - "This is similar to what for example HotSpot or V8 does, and we believe it’s the only time a Ruby JIT has been attempted without any separate supporting compiler framework."
      - "This attempt didn’t go beyond these initial prototype commits."
      - "One other approach was also explored in early 2011. Phoenix began to write code for a custom IR for Ruby, implemented in Ruby. This would have run to apply high-level global optimisation passes before LLVM IR was generated. This example phase removes redundant loads of local variables that have already been read into a register and have not been later modified, and the register is still live, by copying the original register instead of issuing a new load."
      - "The IR did not go beyond this prototype.

        Phoenix handed over leadership of Rubinius to Shirai in 2012, making his last commit at the end of that year."
      - "The Rubinius JIT never really achieved the potential it had in its earlier years. Their initial approach used ideas which had been already been proven historically by Smalltalk and Self, and were being reproven at the time by PyPy, with their RPython translator and later their JIT. The walking back from the idea of using Ruby for everything meant that the VM and large number of runtime primitives needed to support the Ruby core library became a substantial C++ codebase that was not visible to the JIT for optimisation and so the ideas they were building on did not apply in the same way.

        The Ruby IR idea from 2011 was in retrospect probably the missing piece of the Rubinius JIT. It has been shown many times that it’s not realistic to emit LLVM directly from a language AST or bytecode and expect it to apply the optimisations needed to remove abstraction, except in the case of low-level language like C or C++. A custom IR is needed before LLVM IR is emitted in order to express language-specific semantics which LLVM is not aware of. For example, Rust uses MIR, and FTL only emits LLVM after their own CPS and SSA phases and representations. Rubinius needed the Ruby IR.

        Why was the Rubinius JIT never written in Ruby? Over four different efforts, and starting from an initial goal of Ruby in Ruby, they always chose C++ over Ruby for this component."

** TODO jruby

https://ruby-compilers.com/jruby/


* C And C++ Interops

Linking/Including C++ libs into a C host program is possible but not advisable due to *the non-standardization of name-mangling*, however you can "spend a few days writing a C-style wrapper for the C++ library."
  - the other way around, i think works fine though(??)


> C++ can call C functions, `extern "C"` exists.
> C cannot call C++ function "out of the box".

"...for every single function (and all the overloads of that function).":
#+begin_src cpp
extern "C" foo_some_method(Foo &foo) {
  foo.someMethod();
}
#+end_src


