

this was a very messy paste-bin..

* a thought from elsewhere

It seems that from simply wandering around YouTube, starting with Jonathan Blow, moving on to videos from HandMadeCon, I’ve learned quite a lot about the theories of computers: a history of technology, it’s problems—which are current problems—, programming concepts and ideologies, how it relates to hardware, programming languages, compilers, and so on.

Similarly, it seems that from simply wandering around GitHub, I’ve learned quite a lot about how to make stuff: how to parse text, which leads to creating a mark-up language, which itself leads to creating a programming language that trans-piles down to another high-level language; common game engine paradigms, what *good programming is—just fucking doing it: just doing what needs to be done, then, later, refactoring it down to sensible chunks*: “data-driven”; pop game and i/o libraries (SDL, SFML, Haxe frameworks (Kha, Heaps, Flash implementations), MonoGame...); how to use Haxe for everything: generate a web-site (can transpile to JavaScript) with node.js, use C/C++ libraries via ‘extern’, wrap other libraries: thus doing everything—generate front-end web code, serve the internet via back-end web code, make games with game engines (heaps) or multi-media libraries (kha, nme, openFL...) or “foundational” libraries (lime), make cross-platform (native desktop, native mobile, web) applications with


* some basic overviews

from notes from another file

structured programming - divide into subproblems
object oriented programming - identify objects, determine how they interact, specify members and methods

stopped. Just keep the damn notes.


* highlights 'n notes of various videos

** game dev data-oriented design (dod) videos
*** jblow at handmadecon 2015


TODO: move to [game programming]?

https://www.youtube.com/watch?v=Jpkrx1osuLc
  - casey: the programming is customized to the game design
    - jb: *the mindset is to solve the problem. abstractions are only useful to the extent that they solve that problem, most efficiently, expediently.*
    - the barrier between programming and design is only an abstraction
      - (then proceeds to go into some digression)
  - c: why must you still do most of the programming?
    - jb: it would take a week and a half to do something only i can see in my head immediately, much time wasted to communicating to the team/others. (he's chasing some idea down.. and it requires trying different routes, but only he knows the big picture, of where it's all headed)
  - c: the way you program is fungible, as you don't know where you're going, right?
    - jb: *i don't write code meant to work forever, i just write whatever works for the moment in the simplest way possible, because at a later time, in a year, i'll know better of what is needed*
      - in it was a good metaphor for optimization: pushing all the program like a piece of play-dough on a hardware architecture
  - c: i also often say, **my first intuition is good enough for my first pass, and even if you try to over-think it at that time, you don't have enough information to make a better decision anyway**, so in general i do feel that's a good mind-set, however, if you keep that mind-set, then at a certain point, you end up with something that's a mess, and you can't experiment in it anymore. and so *i feel you have something in you that's able to build in a small simple way yet architect in a large way..*
    - jb: envision what is the day i'm going to swap out this bad code, how will it happen. and just constant reality checks... *the hard parts aren't iterating arrays, it's systems that i didn't know were going to play out.*
    - *complexity is just the number [of?] arcs in a node graph.*
      - *in school they teach you it's about the nodes (objects), but the real complexity and what you about are in the arcs. and the more optimized your program is, the more arcs you have.*
    - intelligent decisions/tradeoffs made on what deserves complexity and what does not, in the bigger picture of the end-goal of making a game (used a precision lathe metaphor to build a product)
  - c: what is keeping the super-structure together? what's keeping the arcs in tact? for example, the save game system worked throughout the entire development. is there some way you've learned, where you can isolate small pieces, that doesn't start screwing up everything else?
    - jb: the saved game system, in particular, was solved in braid, needing to serialize data (all time rewinded), and be flexible
    - *off-comment about lisp, doesn't care about repl*
      - *LOL.. good programmers don't need dynamic run-time magics, including the repl..*
    - **i like keeping things simple** (how he solved data serialization in some summer job at sun microsystems)
    - want to edit a whole world collaboratively? put all entities into seperate text files. and put it into version control. because we know version control systems can handle text files. simple.
      - yep.
    - the game engine is the manual
      - *1/3 of the witness comes from ideas from the game engine*
      - *braid came from the idea that, while iterating over some array, can look at the previous states of an enemy, and can skip some of them*
        - *NOTE: yep, my hunch was right, he got the ideas while programming/making..*
    - goes into an idea of a bunch of if statements vs equations
      - **knowing the game design, if princess does this, do that, vs creating systems, and seeing where that goes**
        - "top-down vs bottom-up design", a whole lot of bottom-up
      - c: you mean create rules, and see what happens?
      - **jb: even lower, just drawing a line, the rules that come from that: what happens when it crosses something, etc. it's an investigation**
        - *HMMM..*
      - c: *so you're investigating something, eventually, maybe, you find that light shines to show something, and then that becomes a part of the rule-set*
        - *..!*
      - jb: *yes, and then you have to decide how important of a part it is, and you re-assess that from time to time--it's importance*
      - *nothing is completely bottom-up. top-down is used as a guide, toward something intersting in humans. you have to have a joy in systems, and how systems affect people, and have to be able to take that design and show it, and to communicate it.*
        - *THIS*
    - c: how do you manage everything? do you one part of you that says to do programming, another design, another team, another money, etc.
    - jb: *programming, 99% of the time is stupid things that must be done, and doesn't require much thought as an experienced programmer. that's because the tools we have suck. even optimization, is likely just cache misses.*
    - **code re-use is not a thing, if it were, then we'd move on to bigger and bigger problems. instead, we everything from the past matters...**
    - *on the design side, i tried to make things that are at least new, and my understading as a designer does not kick out with answers to everything immediately* (c: kind athe opposite of the programmer side)
    - *the top-down things says, "oh that's interesting, lets go investigate there", then lets drive in that area to see if anything comes bottom-up... but sometimes, the top-down is strong, and i want to make something, and it doesn't feel good, and i must procedurally try things until i drill my way there*
    
    
*** TODO: 2014, Mike Acton of Insomniac Games


- from a comment:
  - "*I think the key point hes trying to get across is batch your [data] transforms. Identify where the bulk of the calculations are being applied and organise your data so that only the data that is necessary as input is congruent in a data structure to minimise cache misses to generate the output data as efficiently as it can*"
    - this is a great summary..


https://www.youtube.com/watch?v=rX0ItVEVjHc
  - no to all of these: (how are games like on mars?)
    - exceptions (turn off)
    - rtti (run-time type information)
    - iostream (only when debugging)
    - templates (greatly builds up compile times)
    - operator overloading
    - multiple inheritance
    - no STL
      - not that we have anything better, but it doesn't solve the problems we have
    - *custom allocaters (tons)*
      - don't use the general heap management
      - sandboxed..?
    - *custom debugging tools*
      - to *introspect data, examine live, offline, analyze, etc.*
  - 12:41, his meaning of DoD
    + **the purpose of all programs, and all parts of those programs, is to transform data from one form to another**
    + *if you don't understand the data, you don't understand the problem*, and conversely:
    + *understand the problem by understanding the data*
    + different problems have different solutions, so:
    + *if you have different data, you have a different problem*
    + ...
    + *everything is a data problem*
    + ... rule of thumbs ...
    + DoD is mostly a response to the broader [normative] culture of C++
    + *3 big lies*:
      - software is a platform
        - obviously, hardware is..! (*hardware is the platform*)
        - **text + data -> cpu -> data**
          - simple idea of what programs do
      - code should be designed around model of the world, some relation to real data, some idealized form
        - *hiding data: bad*
        - confounds two things:
          - maintenance (via controlling access)
          - understanding properties of data
        - solve by analogy, feels like story-telling, a childish way to go about programming
      - code is more important than data
        - *solving data transformation is the goal, not writing code*
        - *there is no ideal abstract solution to the problem*
        - **you can't future proof**
          - this reflects what jblow was saying, though, in a far more blunt, less introspective/poetic way.. lol
    + hash
      - **most of the time just iterating until you find something, some one thing, a single hit, the rest are cache misses**
      - **WRONG.**
      - *the most common things needed should come first: the keys you most commonly use*
        - TODO: don't hash tables optimize for this..??
    + a good diagram on *cpu->cache / cpu---------->ram* (1 dash = 20 cycles)
      - gameenginebook.com/sinfo
      - *l1 cache 3 cycle*
      - *l2 cache 20 cycles*
      - *ram 200 cycles*
    + ~33min, *incredible bit about what an object in OOP actually looks like in assembly, and the cost in time in latency*
      - TODO: gotta re-wath this vid..
      - **10 cache misses to 1, 3 going to ram**
        - **that 1 is what the compiler optimized, the rest are human errors**
      - changes it to DoD
        - *can then kinda guess/calulate cacheline stuff from looking at the code*
        - *10x speed-up*
    + common problems:
      - bools in structs
        - not only wastes 7 bits in a struct, but mainly, pushes the cacheline 1 byte, possibly costing another read
        


**** some cache research

***** how cache works
https://www.nic.uoregon.edu/~khuck/ts/acumem-report/manual_html/ch03s02.html
  - allocation of space in the cache is managed by the processor. When the processor accesses a part of memory that is not already in the cache it loads a chunk of the memory around the accessed address into the cache, hoping that it will soon be used again.
  - The chunks of memory handled by the cache are called *cache lines*. The size of these chunks is called the *cache line size*. Common cache line sizes are 32, 64 and 128 bytes.
  - A cache can only hold a limited number of lines, determined by the *cache size*. For example, a 64 kilobyte cache with 64-byte lines has 1024 cache lines (cache size * cache line size)
  - If all the cache lines in the cache are in use when the processor accesses a new line, one of the lines currently in the cache must be evicted to make room for the new line. The policy for selecting which line to evict is called the *replacement policy*.
    - The most common replacement policy in modern processors is for least recently used (LRU)
      - hmm, simple enough.. lol
  - When a program accesses a memory location that is not in the cache, it is called a *cache miss*.

***** a bit more
https://stackoverflow.com/questions/3928995/how-do-cache-lines-work
  - "If the cache line containing the byte or word you're loading is not already present in the cache, your CPU will request the 64 bytes that begin at the cache line boundary (the largest address below the one you need that is multiple of 64).

  *Modern PC memory modules transfer 64 bits (8 bytes) at a time, in a burst of eight transfers, so one command triggers a read or write of a full cache line from memory*. (DDR1/2/3/4 SDRAM burst transfer size is configurable up to 64B; *CPUs will select the burst transfer size to match their cache line size, but 64B is common*)
  
  As a rule of thumb, if the processor can't forecast a memory access (and prefetch it), the retrieval process can take ~90 nanoseconds, or ~250 clock cycles (from the CPU knowing the address to the CPU receiving data).
  
  By contrast, a hit in L1 cache has a load-use latency of 3 to 5 cycles, and a store-reload has a store-forwarding latency of 4 or 5 cycles on modern x86 CPUs. Things are similar on other architectures."
  
  - "100ns (to access RAM) means about 200 ticks (clock cycles). So basically **if a program would always miss the caches which each memory access, the CPU would spend about 99,5% of its time (if it only reads memory) idle waiting for the memory."**
    - **this souhnds like the greatest bottle-neck of pc performance, especially regarding games: memory access**
    



*** a really good video on the pitfalls of OOP and it's solution: DoD, using animation in chromium as an example

https://www.youtube.com/watch?v=yy8jQgmhbAU
  - at 32:00, there's a good comparison chart:
    - inheritance of six classes, with everything crammed into a single object ("Animation"), with links/references everywhere
      - vs a templated struct (AnimationState)
        - **tempalates are a sort of static polymorphiwsm**
    - references of key-frame data
      - vs read-only duplicate of key-frame data
    - list of dynamically allocated interpolations: abstract classes
      - vs vectors (dynamic array)  per-property, or if you know the types, an array
    - flags (or some if branch) on state
      - vs different tables (vectors) for each state
        - notably, the state is hidden within a class/object
    - an api wrapper around the object
      - vs a new simple api based on the things the user needs: the id of a data
    - outputs new property value to element and marks the hierarchy of elements (DOM sub-tree) (interacting with some other system)
      - outputs tables of new values/modified elements (no interaction with any system, the data is simply passed down the pipeline to the next system))
      
  - 33:55 key points: try to do the following:
    - **keep data flat**
      - hardware reasons (cache, no rtti, amortized dynamic allocations)
      - some read-only duplication improves performance and readability
        - this bit often happens in functional paradigm, which may seem wrong as the data is copied, but perhaps is nicer for the hardware, as the data is in one place, not mutating depending on the state
    - existence-based predication
      - (to get rid of state all over the place)
      - reduce branching (if and case statements)
    - use ID-based handles
      - reduce pointers
      - simplify life-time of objects
      - by seperating what the user sees, allows the implementation to be re-arranged as needed
    - **use table-based output**
      - (which will be re-used as input down the pipeline)
      - no external dependencies
      
  - result: 6x better performance
    - no change in algorithm or anything, just how data is structured
    
  - stopped at 35:00
    - OOP makes multi-threading really tough
    
  - skimmed through it..
    - OOP is not simply classes and methods, DoD uses them too
      - the differences is that DoD puts data first, not the operations / code
    - in games, DoD goes hand-in-hand with entity-component-systems
    - "know your machine, and design software for the machine that will run it"
      - and "don't design for an abstract hypothetical machine"
    - in the end, both OOP and DoD are just tools in your toolbox, it is up to your best judgement when to use them
    
    
    
    
  - from a comment
    - "The speaker could have given a much better answer to that second question, which asked about what exactly he thinks should be "dead" in OOP.

    OOP was designed to help with program design, maintainability and reusability. Things like encapsulation and abstraction are key core concepts of OOP, and they were developed in order to aid in the design of huge programs. It's a tool to create a million-lines-of-code program in such a manner that it remains manageable, understandable and maintainable. When properly used, it makes code simpler, safer and easier to understand and to develop further. It also helps in making code reusable, meaning that it's relatively easy to take some functionality that could be used in another part of the same program, or even a completely different program, and use it there without much or any modification. This helps reducing code repetition and overall work. It also helps in stability and reducing the number of bugs, because when a piece of code has been extensively tested, you can be certain it won't be a problem when used in another place or program. OOP does a relatively good job at this.
    
    The problem with OOP is that it wasn't designed, at all, with low-level efficiency in mind. Heck, back when OOP was first developed as a paradigm computers didn't even have any caches, pipelines or anything like that. There were no "cache misses" because there were no caches. Memory access speed was pretty much constant no matter how you read or wrote there. The performance penalty from conditionals and branching wasn't such a big concern back then either. It was but decades later that processor architectures went into a direction where the scattering of all the data randomly in memory became an efficiency problem.
    
    Thus, **if we want maximum efficiency in terms of cache usage, what needs to "die" in OOP is the concept and instinct of decentralizing the instantiation of the data that has a given role. The data needs to be taken out of classes, and centralized into arrays, and thus we need to break encapsulation and modularity in this manner. We also need to minimize branching, which in terms of OOP means minimizing the use of dynamic binding (in addition to try to minimize conditionals).**"
    
    
  - another comment:
    - "Lots of OOP fans are being triggered here, lol. **Basically, it boils down to memory access patterns. If you can arrange your computations to simply "flow", best without hiccups, i.e. without branching and chasing pointers all over the place, and if you can put data being accessed by hot code in the same place, so they can be prefetched in the cache, you win big, performance-wise. That is what DOD is all about: to better match program data to the way hardware operates.** Now, in cold code, OOP is just fine: it dramatically increases programmer efficiency at the cost of runtime efficiency, a cost we're willing to pay. But in hot code, HPC code, and real-time code, DOD is vastly superior, as it much better matches the problem to the hardware."

  - another comment:
    - "Good talk. I hope that some viewers notice that **the real benefit of this approach is how this makes testing, scaling, and modifying the code a lot easier, because of the loose coupling, reduced complexity, and freedom to reorder the algorithm. Performance is just the cherry on the cake.**
    
    

  - https://en.wikipedia.org/wiki/Data-oriented_design
    - "Although OOP appears to "organise code around data", it actually organises source code around data types rather than physically grouping individual fields and arrays in an efficient format for access by specific functions. Moreover, it often hides layout details under abstraction layers, while a data-oriented programmer wants to consider this first and foremost."
    - structure of arrays > array of structures
  - anti-object-oriented programming
  - see Handmade Hero or some jblow video
  - basically: just solve the problem. Don't try to fit things into logical abstractions. Abstractions have no real functionality. Abstractions are for us, humans, the computer doesn't care.
  - when things are programmed quickly, that's basically data-driven design. The results of game jams, quick solutions, etc. The shit that gets things done. Usually done in one large file, right there in the main loop. No need for classes or separate files. Just structs as needed, right there, in the same file.
  - instead of objects, there are just bundles of data. It's a lot more vague, but a lot more flexible, as it isn't limited by trying to fit some abstract object. The vague bundle of data it is, is actually a much neater solution than writing a bunch of abstractions.


** Learned From The Last Decade Of Software Dev for Web, Mobile, Games, and Quantum Computing by Amir Rajan

(maker of dragonruby)

new:
https://www.youtube.com/watch?v=7xsX2JP-Xxo
  - 2010:
    - "a website without javascript is the best self-documenting rest api you can build"
  - 2011:
    - "*a language cannot be slow, a runtime can*"
      - C# has several runtimes (pre-core, core, mono, unity, etc.)
        - hmm, didn't know unity had it's own..
    - C#. 4.5/5: lambas (and linq) changed the game
      - i remember this, and agree!
  - 2012:
    - JS frameworks
      - react
        - request-animation-frame() (basically game tick() at 60hz)
      - nodejs
  - 2013:
    - OSS burnout (companies should pay free software makers!)
    - a simple cell-based grid layout simplifies mobile dev (24x12, 16:9)
  - 2014:
    - polygot (clojure, haskell, F# (m$'s ocaml), scala, ruby)
  - 2015:
    - made "a dark room" for web, then used ruby motion to port it, and it sold 1mil+, then made a prequel that also sold well
  - 2016:
    - bought/aquired the tool-set/company he used to make the game: rubymotion from a guy who worked at apple and wanted to retire anyway, and re-named it dragonruby
    - new space in tech: how was rubymotion built? previously he always used frameworks to make stuff. it's a runtime
  - 2017-2018: the realization:
    - study in isolation: how to build a runtime
    - the open-source burnout year of 2013 provided the experience needed
    - describes llvm / chris latiner:
      - 2009: chris is at MIT, then goes to apple, when the year the iphone is released, using llvm
      - *llvm is an infrastructure that seperates/decouples the langauge from the hardware*
        - *langauge -> IL/R -> bitcode/machine*
      - 2010: *iphone is a hit, intel wants to build chips for it, apple says you must make the chipset architecture compatible to llvm*
        - *IR -> opcodes for a chipset architecture (supplied by the hardware manufacturer) -> bit-code*
      - 2016: chrome switches to clang (the c compiler for llvm)
      - 2019
        - .net core (+clr) is released, which is basically a C# runtime implemented on llvm
          - (wow.. i didn't know this..)
          - a major turning point for microsoft, as they had the competing c++ runtime, visual c++/msvc, for a long long time
        - android 26 uses clang
          - chris goes to tesla, which therefore uses clang
          - *every giant company uses clang, and basically the entire world runs on clang*
      - 2020: *wasm builds a back-end for clang*
      - 2021: apple m1 comes out
        - *it [apple m1] performs well because the ir generated by llvm directly maps to m1 arch op-codes, they were able to make it so due to the ubiquity of clang (and x86 arch)*
          - TODO: hmm, read again..
    - *the realization: build run-time on clang*
      - *made the game toolkit for rubymotion/dragonruby runtime*
  - 2019-2020:
    - *port games to nintendo switch, which also uses clang*
      - thus, *the crazy coincidence: the games he made back in 2012 on rubymotion, through a history of tech, works on nintendo switch*
  - quantum computing
    - there are applications to encryption, but what about runtimes?
	  
* seems to be looking into game specific abstractions
  
object pool
  - http://www.haxeflixel.com/articles/bullet-manager-part-1

https://github.com/richardlord/Asteroids/tree/master/src/starling/net/richardlord/asteroids
  - why entity is overkill for 2d games without an editor like Unity

http://www.richardlord.net/blog/what-is-an-entity-framework
  - really good read

inheritance vs composition vs components
  - http://www.learn-cocos2d.com/2010/06/prefer-composition-inheritance/
  - http://stackoverflow.com/questions/7085130/should-i-subclass-ccsprite-ccnode-or-nsobject

dx, dy = -dy, dx; //simultaneous assignment, can use this to swap variables

before entering data into a database, ensure it is how you want it, add error handling (try/catch)

0x10 = 16, 0x20 = 32 ...

COM, interface, loose coupling - http://www.codeproject.com/KB/COM/COMBasics.aspx
also can inherit an interface to extend it. For example, ISprite, ISuperSprite, I SuperDuperSprite.





* from _temp.txt

vocab (C++)

const data
  - stores string literals and other data whose values are known at compile time
stack
  - stores automatic variables. typically allocation faster than dynamic storage (heap or free store). constructed/destroyed immediately after allocation/deallocation.
free store
  - dynamic memory allocated/freed by new/delete. object lifetime can be less than allocation time; objects can be allocated without being initialized or destroyed without being deallocated.
heap
  - another dynamic memory area. allocated/freed by malloc/free and their variants.
global/static variables and objects
  - ?
malloc
  - subroutine for performing dynamic memory allocation


* possibly from my school days / C++ / basic concepts, data structures, etc.

a lot of this probably belongs to [programming-language-concepts]..

http://www.joelonsoftware.com/articles/fog0000000043.html - the joel test

http://en.wikipedia.org/wiki/Creational_pattern

reflection - In computer science, reflection is the process by which a computer program can observe (do type introspection) and modify its own structure and behavior at runtime. http://en.wikipedia.org/wiki/Reflection_%28computer_science%29

http://en.wikipedia.org/wiki/Object_copy
A good answer about copying objects - http://stackoverflow.com/questions/3913189/as3-impossible-to-copy-displayobjects-with-content


Purely functional - a term in computing used to describe algorithms, data structures or programming languages that exclude destructive modifications (updates). According to this restriction, variables are used in a mathematical sense, with identifiers referring to immutable, persistent values.

interface - A interface defines, which method a class has to implement.
This way - if you want to call a method defined by an interface - you don't need to know the exact class type of an object, you only need to know that it implements a specific interface.

data structures
http://lab.polygonal.de/as3ds/
array
multi-dimensional array -
queue - FIFO (first in first out), not really a structure, just a way of accessing not store
stack - FILO (first in last out)

tree - node-based structure. Every tree starts with a single node (the root node). The root node can contain children, and that node can contain children and so on.
binary Tree - a specific type of tree, in which each node can only have upto two children
Binary Search Tree - stores data that can be retrieved quickly with a key, like a hash table, except BST uses a recursive apporach
linked list
heap - loose node-based structure in which every node is larger than its child nodes
priority queue - builds upon the heap
hash - builds upon an array
graph - loose node-based structure. Nodes are connected by arcs. Used for path-finding
bit vector - an array of bits, very efficient

linked list vs dynamic array - Linked lists have several advantages over dynamic arrays. Insertion or deletion of an element at a specific point of a list, assuming that we have a pointer to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation, whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case. While one can "delete" an element from an array in constant time by somehow marking its slot as "vacant", this causes fragmentation that impedes the performance of iteration.

linked list vs array - The principal benefit of a linked list over a conventional array is that the list elements can easily be inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk. Linked lists allow insertion and removal of nodes at any point in the list, and can do so with a constant number of operations if the link previous to the link being added or removed is maintained during list traversal.
On the other hand, simple linked lists by themselves do not allow random access to the data, or any form of efficient indexing. Thus, many basic operations  such as obtaining the last node of the list (assuming that the last node is not maintained as separate node reference in the list structure), or finding a node that contains a given datum, or locating the place where a new node should be inserted  may require scanning most or all of the list elements.

-Arrays assume every element is the same size, often limiting the data types to the primitives
-linked lists can be sorted

list
get is O(n)
add is O(1)
remove is O(n)
Iterator.remove is O(1)
For ArrayList

array
get is O(1)
add is O(1) amortized, but O(n) worst-case since the array must be resized and copied
remove is O(n)

method overriding - features that allows a child class to provide a specific implementation of a method that is already provided in one of its parent classes
method overloading - creating several methods with the same name that differ from each other in terms of input and output
polymorphism - the ability to create a variable/function/object that has more than one form. Examples: operator overloading, late-binding, dynamic-binding
dynamic binding - the process of mapping a message to a specific sequence of code (the method) at runtime. Examples: Objective-C, Smalltalk, Javasript and Python too?
unified type system - hence ToString, all data types inherit from a base class
interface - ?


conceptual knowledge > language specific
for language specifics, use the internet, or a cookbook



The shell is the program which actually processes commands and returns output. Most shells also manage foreground and background processes, command history and command line editing. These features (and many more) are standard in bash, the most common shell in modern linux systems.

A terminal refers to a wrapper program which runs a shell. Decades ago, this was a physical device consisting of little more than a monitor and keyboard. As unix/linux systems added better multiprocessing and windowing systems, this terminal concept was abstracted into software. Now you have programs such as Gnome Terminal which launches a window in a Gnome windowing environment which will run a shell into which you can enter commands.

The console is a special sort of terminal. Historically, the console was a single keyboard and monitor plugged into a dedicated serial console port on a computer used for direct communication at a low level with the operating system. Modern linux systems provide for virtual consoles. These are accessed through key combinations (e.g. alt+F1) which are handled at low levels of the linux operating system -- this means that there is no special service which needs to be installed and configured to run. Interacting with the console is also done using a shell program.

abstract data type - in theoretical computer science, a mathematical model for certain class of data structures that have similar behavior

tuple - an ordered list of elements, like the index of arrays

http://en.wikipedia.org/wiki/Container_(data_structure)
array - similar to vector, fixed number of indices, indices can be computed at runtime
	matrix - an array with two indicies
list or sequence - ordered collation of values
map (associative array, dictionary, index) - add, reassign, remove, lookup. Indexed by keys instead of a range of numbers. Lookup O(1).
	hash table
	self-balancing binary search tree -
	b-tree -
queue - first in first out, insert from the back, pop from the front,
	double ended queue (deque) - elements can be added or removed from the front or back
	priority queue - insertWithPriority (set a priority) and pullHighestPriorityElement
set - a hash of items, no index
stack - last in first out, insert to front, pop from front
string - a finite sequence of symbols that are chosen from a set (i.e. alphabet)
table
tree - hierarchical tree structure with a set of linked node, acyclic
vector

sorting
bubble
heap


associative array - like array, except the index can be any data type, not just integer. The most important operation is lookup - finding the value associated to the key. the lookup is optimized by memorization.

hash function - converts large data into a small data, usually an integer, that may serve as in index (cf. associative array).

hash table...
the average cost for each lookup is independent to the number of elements stored in the table





	interpreted language - execute without compilation
	has structured programming syntax (if, while, etc.)
	no scoping, use 'let'
	dynamic typing
	object based
	case sensitive

strongly typed
garbage handling - don't need to destruct classes
no need for function prototypes

during SDLC, there are four types of errors:
Machine errors				processor dependent, ex. 1/3=3.3333, the processor must truncate it
Compile-time errors 		easy to find out, mistakes in syntax
Run-time errors 			occurs during program execution, usually due to uninitialized variables
Logical and design errors	the programmer's fault, maybe wrong order of operation, ex. If ( ! recSet.IsEOF() && recSet.IsBOF() ) instead of If ( ! ( recSet.IsEOF() && recSet.IsBOF() ) )

debug and release builds
debug - Full symbolic debugging information in Microsoft format No optimization
release - No symbolic debugging information , Optimized for maximum speed

purely functional
a term in computing used to describe algorithms, data structures or programming languages that exclude destructive modifications (updates). According to this restriction, variables are used in a mathematical sense, with identifiers referring to immutable, persistent values.
for example: if you wanted to add a node far down a binary tree, you would have to create many duplicate nodes before adding the node
garbage collection is commonly found to free up nodes which have no live references

lazy evaluation (opposed to eager)


encapsulation
in an object-oriented programming language, encapsulation is used to refer to one of two related but distinct notions, and sometimes to the combination[1][2] thereof:
A language mechanism for restricting access to some of the object's components.[3][4]
A language construct that facilitates the bundling of data with the methods operating on that data.[5][6]
Programming language researchers and academics generally use the first meaning alone or in combination with the second as a distinguishing feature of object oriented programming. The second definition is motivated by the fact that in many OOP languages hiding of components is not automatic or can be overridden; thus information hiding is defined as a separate notion by those who prefer the second definition.

or hiding implementation


loose coupling
Coupling refers to the degree of direct knowledge that one class has of another. This is not meant to be interpreted as encapsulation vs. non-encapsulation. It is not a reference to one class's knowledge of another class's attributes or implementation, but rather knowledge of that other class itself.

Strong coupling occurs when a dependent class contains a pointer directly to a concrete class which provides the required behavior. Loose coupling occurs when the dependent class contains a pointer only to an interface, which can then be implemented by one or many concrete classes. Loose coupling provides extensibility to designs. A new concrete class can easily be added later that implements that same interface without ever having to modify and recompile the dependent class. Strong coupling does not allow this

inheritence

abstraction

object oriented programming

procedural programming

information hiding
The principle of segregation of design decisions in a computer program that are most likely to change, thus protecting other parts of the program from extensive modification if the design decision is changed

encapsulation (used interchangably with information hiding)
A language mechanism for restricting access to some of the object's components.
A language construct that facilitates the bundling of data with the methods operating on that data

Hiding the internals of the object protects its integrity by preventing users from setting the internal data of the component into an invalid or inconsistent state. A benefit of encapsulation is that it reduces system complexity and thus increases robustness, by limiting the interdependencies between software components.

the process of compartmentalizing the elements of an abstraction that constitute its structure and behavior; encapsulation serves to separate the contractual interface of an abstraction and its implementation

example - Encapsulating software behind an interface allows the construction of objects that mimic the behavior and interactions of objects in the real world. For example, a simple digital alarm clock is a real-world object that a lay person can use and understand. They can understand what the alarm clock does, and how to use it through the provided interface (buttons and screen), without having to understand every part inside of the clock. Similarly, if you replaced the clock with a different model, the lay person could continue to use it in the same way, provided that the interface works the same.



software development methods
agile
waterfall

const
Like most keywords in C++, the const modifier has many shades of meaning, depending on context. Used to modify variables, const (not surprisingly) makes it illegal to modify the variable after its initialization.
Thus, const can replace the use of the #define to give names to manifest constants. Since preprocessor macros don't provide strong compile-time type checking, it is better to use const than #define. Moreover, some debugging environments will display the symbol which corresponds to a const value, but for #define constants, they will only display the value.
The const keyword is more involved when used with pointers. A pointer is itself a variable which holds a memory address of another variable - it can be used as a "handle" to the variable whose address it holds. Note that there is a difference between "a read-only handle to a changeable variable" and a "changeable handle to a read-only variable".

What's the difference between a definition and a declaration?
simply: declaration tells the program to expect a function, whereas a function actually reserves memory for it. "a forward reference"
You have to declare functions, structures and variable because the compiler needs to know how much memory (or stack space) to reserve for these things (based on type, size, parameters).
int myFunction(int x, char c) //function prototype. tell the compiler myfunction is going to use 5 bytes, when it is defined.
extern int a //declaration for global variable.
definition = declaration + space reservation
int myFunction(int x, char c){return c+x;} //function definition.
int a //definition

taken from - http://www.mactech.com/articles/mactech/Vol.09/09.10/CPPBasics/index.html

In the C language, the struct is the most sophisticated bundling mechanism available.

const vs #define - You may have noticed the use of const instead of a #define. Believe it or not, const is part of the ANSI C standard and is not just found in C++. Though many C programmers prefer to use #defines to define a constant, C++ programmers always use const. The major advantage of using const is that a typed constant is created. If you pass a const as a parameter to a function, for example, C++'s parameter checking will ensure that you are passing a constant of the correct type. A #define just does a simple text substitution during the first pass of the compiler.

classes bundle data members and functions together. data members are allocated together, but the functions point to the real function allocated elsewhere.

creating objects
object myObject		definition objects, memory is allocated (on the stack) and deallocated automatically

Although automatic objects are simple to create, they do have a downside. Once they drop out of scope, they cease to exist. If you want your object to outlive its scope, take advantage of C++'s new operator (and delete).
'new' takes a type instead of a number of bytes.

object* objectPtr	defines an object pointer. pointer declaration?
objectPtr = new object	creates an object. allocates memory (on the heap/free store) for your object. returns a pointer to the newly created object [requires above statement]

if you have 2 new objectPtr, both objects have it's own copy of the object's data members, but the function pointers point to the same function.

pointer objects should always be deleted, regardless of which control path is taken, or if exceptions are thrown. memory will leak if pointers objects are not deleted

stack - where all local variables go. limited memory, if you allocate too many objects on the stack, you risk stack overflow
heap/free store - where objects go.

If you'd like to return a pointer to your object from a function, you must use new

employeeSalary = 400; //in a class function, it's the same as this->employeeSalary = 400;

Just as a variable is based on a type definition, an object is based on a class definition.

the constructor is C++'s built in initilization for classes. generally, it initializes each data member. when you create an object, the constructor is invoked (if it exists). it's not a good idea to allocate memory inside your constructor* cannot return a value.
"two stage construction" - Employee employee; employee.init(); init can be used to allocate memory, or perform other initializations that could fail. could then check the return status of init(), and delete the object if init fails.

destructor is called when you use "delete object;". for automatic objects it is called when the function ends, or when you call ~object();
destructor is used deallocate any additional memory your object may have allocate

global variables and objects are constructed at the beginning of the program and destructed at the end of the program

Whether you use two-stage initialization or not, it's a good idea to keep your constructor and destructor in sync. If you allocated extra memory, be sure your destructor has some way of knowing about it. For example, it's good practice to initialize your pointers to null. If your destructor encounters a non-null pointer, it knows that additional memory has been allocated that must be deallocated.



char	1 byte (1byte = 8 bits = 2^8 = 256)
short	2
int		4 (signed: -2147483648 to 2147483647)
long	4
float	4 bytes (~7 digits)
double	8 bytes (15-16 digits)
decimal	16 bytes 28-29 decimal places

Escape Characters
\'	 Single Quote
\"	 Double Quote
\\	 Backslash
\0	 Null, not the same as the C# null value
\a	 Bell
\b	 Backspace
\f	 form Feed
\n	 Newline
\r	 Carriage Return
\t	 Horizontal Tab
\v	 Vertical Tab

Category (by precedence)	 Operator(s)	 Associativity
Primary	 	x.y  f(x)  a[x]  x++  x--  new  typeof  default  checked  unchecked delegate	 left
Unary	 	+  -  !  ~  ++x  --x  (T)x	 left
Multiplicative	 *  /  %	 left
Additive	 +  -	 left
Shift	 	<<  >>	 left
Relational	 <  >  <=  >=  is as	 left
Equality	 ==  !=	 right
Logical AND	 &	 left
Logical XOR	 ^	 left
Logical OR	 |	 left
Conditional AND	 &&	 left
Conditional OR	 ||	 left
Null Coalescing	 ??	 left
Ternary	 ?:	 right
Assignment	 =  *=  /=  %=  +=  -=  <<=  >>=  &=  ^=  |=  =>	 right

associativity, operations are read left to right, assignments are evaluated right to left

pre-increment/decrement vs post-increment/decrement


guidelines
use 'this'
don't use 'using', write out the namespace path

preprocessors
#include - includes the file, but if a file is includes twice, it will cause a compilation error

include/macro guards:
should always use them. be sure to have a coherent naming scheme, so you don't duplicate include guard names

#ifndef FILESYSTEM_H
#define FILESYSTEM_H
...
#endif

#pragma once - ensure the file will be included once. C++ only, still has a fault
#import - the best implementation. like include, but only includes a file once. - objective-c/java/python

forward declaration:
use it when you want to use a reference or pointer to an object
don't use it when you want to use an object or inherit an object
http://www.eventhelix.com/realtimemantra/HeaderFileIncludePatterns.htm

A.h
#include "abase.h"
#include "b.h"
class C;
class D;
class A : public aBase {B bee; C* cee; D* dee}

A.cpp
#include "a.h"
#include "b.h"
#include "d.h"

void A::setC(C* c){cee=c}
C* A::getC() const{return cee}
void A::modifyD(D* d){d->SetX(0); dee=d;}

A header file should be included only when a forward declaration would not do the job. - forward declaration
The header file should be so designed that the order of header file inclusion is not important.
The header file inclusion mechanism should be tolerant to duplicate header file inclusions. - include guards

control statements:
if/else
switch - more optimal then a long chain of if else
break
continue - skip following remaining statements, and continue to next iteration
goto

loops:
while
do/while - A do loop is similar to the while loop, except that it checks its condition at the end of the loop. This means that the do loop is guaranteed to execute at least one time. example - presenting a menu
for - includes initialization of a iteration variable
foreach(<type> <iteration variable> in <list>) { <statements> } - used on arrays or collections, ex. foreach(string person in school){Console.WriteLine(person)}, person is each element

methods: - seperate logic into units, can pass information, can return values

accepting command-line input
for example - Notepad.exe MyFile.txt //MyFile.txt is the parameter
static void Main(string[] args)
args[0] //=MyFile.txt

to use a class's methods, you must instantiate (create) an object

static vs instance (normal)
instance classes can have mutliple instances
static classes only have one definition
static objects can be called at anytime
instance objects must be instantiated before using its methods

OneMethod om = new OneMethod(); //instance of the oneMethod class references by om


4 parameters - out, ref, params, value

value - return data;
ref - void function (ref object1) - changes to local object1, changes the caller reference's object
out - used to return multiple variables
params - allows methods to accept a variable number of arguments ex. void viewAddresses(params string[] names)

namespaces:
used to help organize programs
usually contains multiple data structures (classes, structures, interfaces, enumerations, delegates)

can have nested namespaces
namespace1{namespace11{class1{}}}
namespace1.namespace11{class1{}} //shortcut

namespace1.namespace11{class2{}} could be added in another file!

Namespaces don't correspond to file or directory names. If naming directories and files to correspond to namespaces helps you organize your code, then you may do so, but it is not required.

using - using namespace1.namespace11 - allows you to call methods without typing the 'fully qualified name', the full namespace path
alias - alias n11 = namespace1.namespace11 - shorten a long namespace

classes
constructor - used for initializing class memeber, always has the same name as the class, cannot return a value, not mandatory
can have multiple constructors, calls depend on the number/type of parameters
instance and static members (fields and methods) - static members must be called via className.staticMethod(); as opposed to objectName.method();, used in calculations or anything that doesn't require an intermediate step
changing default constructor - public OutputClass() : this("Default Constructor String") { }, ensures fields are initialized during instantiation

inheritance

polymorphism

properties

indexers

structs

interfaces

delegates and events

exception handling
try...catch - allows you to test a block of code for errors. The try block contains the code to be run, and the catch block contains the code to be executed if an error occurs.
throw - creates an exception. often used in conjunction with try...catch, to create accurate error messages.
try...except...else -

try:
  z=5/0
except ZeroDivisionError: #catches this specific error
  print "divide by zero"

try:
  stuff
except: #catches everything
  stuff

try:
  do_some_stuff()
except:
  rollback()
  raise
else:
  commit()

Sometimes, you want to catch all errors that could possibly be generated, but usually you don't.In most cases, you want to be as specific as possible ("Catch What You Can Handle")

raise - generates exceptions
finally - placed at the end of try..except..else, always runs

attributes

enumerations

overloading operators

encapsulation

collections

anonymous methods

C# type

nullable types






Bar::Bar(int foo, int otherFoo): foo(foo), otherFoo(otherFoo) {...} //That way you know for sure that the foo var is initialized before anything else happens.

vocab
accessors - getters and setters
getters
setter
parsing (syntactic analysis) - the process of analyzing a text, made of a sequence of tokens (for example, words), to determine its grammatical structure with respect to a given (more or less) formal grammar.
regular expression - A regular expression is an object that describes a pattern of characters. can be used for searching, parsing, format checking, and substitution. modifiers: g=global, i=case insensitive, .test(), .exec(), etc.
A command-line interpreter (also command line shell, command language interpreter) - is a computer program that reads lines of text entered by a user and interprets them in the context of a given operating system or programming language.
multitier architecture (n-tier) -
inline function - tells the compiler to insert the complete body of the function in every place in the code where that function is used. higher runtime performance, larger code


runtime/run-time - the time at which the program is executing or--running

process
compile
link
load
execute

type checking - process of verifying and enforcing constraints of types. May occur at compile time or run-time depending whether the language is strongly or weakly typed

static - you can call methods inside it without instantiating (creating an object, using new) it. this is why Main is static. when you do this, the program creates an instance of the class when upon execution.



